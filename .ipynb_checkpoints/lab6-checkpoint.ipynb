{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60c472b-a00e-49e1-a13b-298d3584fbc3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25c41159fbc3b698",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 6 - Retrieval Models: BM25\n",
    "CS 437  \n",
    "Fall 2025  \n",
    "Dr. Henderson  \n",
    "_v1_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23696b4f-ffc7-47b7-9bca-deec5ad6fe4a",
   "metadata": {},
   "source": [
    "The BM25 ranking model is one of the most effective models assuming a binary relevance. It is based on the _binary independence model_ which makes the Naive Bayes assumption of _term independence_, simplifying the calculations.\n",
    "\n",
    "Although the _binary independence model_ is known to have poor performance, we'll begin by implementing it so we can extend it to BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6534ae0-af67-49db-bf44-b13c1905fd60",
   "metadata": {},
   "source": [
    "The score for a document in the _binary independence model_ is given by\n",
    "$$ bim\\_score(d) = \\sum_{i:d_i=1} \\log \\frac{p_i(1-s_i)}{s_i(1-p_i)} $$\n",
    "\n",
    "Where $d_i=1$ means term $i$ is present in the document, $p_i$ is the probability term $i$ occurs in the relevant set, and $s_i$ is the probability term $i$ does _not_ occur in the relevant set.\n",
    "\n",
    "Since we don't know the relevant set beforehand we cannot calculate $p_i$ and $s_i$ directly so we'll use the estimates as described in the book. We'll choose $p_i = 0.5$ (50/50 chance the term occurs in the relevant set), and $s_i = \\frac{n_i}{N}$ (proportion of documents the term occurs in the whole collection $N$) giving us:\n",
    "$$ bim\\_score(d) = \\sum_{i:d_i=1} \\log \\frac{N - n_i}{n_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788e5c1-ee39-4474-8934-e4e11f8e3cab",
   "metadata": {},
   "source": [
    "_Note: We won't preprocess the query or the corpus for this lab to make it simpler_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826478a-d998-498c-8b6e-d3854e793711",
   "metadata": {},
   "source": [
    "### Part 1: Binary Independece Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d936989-e2f6-4217-9d2e-11b387dfec13",
   "metadata": {},
   "source": [
    "1. Create a function called `term_count()` which takes a string and returns the number of documents it occurs in the collection found in the `docs` subdirectory.  \n",
    "   _Hint: In homework 1 you used a system command to quickly search a set of documents for a term_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae5b8561-aaa7-49a5-a255-1cb9794f5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def term_count(term):\n",
    "    grep = subprocess.run([\"grep\", \"--ignore-case\", \"-lr\", term, \"docs\"], text = True, capture_output = True)\n",
    "    return len(list(grep.stdout.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46196f61-7df4-4d01-9e8c-0393f81bccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ut_1 = term_count(\"squirrel\")\n",
    "assert _ut_1 == 2, _ut_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094c683-b58e-4a71-b898-b66dcc4ceba7",
   "metadata": {},
   "source": [
    "2. Set a variable named `N` to the number of documents in the `docs` subdirectory, and state `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9850351d-b8cd-402a-86f0-8a846ec43fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(list(subprocess.run([\"ls\", \"-1\", \"docs\"], text = True, capture_output = True).stdout.split(\"\\n\")))\n",
    "# N = 116\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac15b87-c095-4b67-9c8b-b1476f96b5cd",
   "metadata": {},
   "source": [
    "3. Create a function named `bim_score()` which takes a document filename and a list of query terms and returns the BIM score for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8d6345-2ce7-4c60-aff9-0d28171b62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def bim_score(d, terms):\n",
    "    score = 0\n",
    "    for term in terms:\n",
    "        if subprocess.run([\"grep\", \"--ignore-case\", \"-q\", term, d], text = True).returncode == 0:\n",
    "            n = term_count(term)\n",
    "            score += math.log((N-n)/n)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "911687df-1ac4-40a0-8aa3-5a1ed8ef2365",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect score: 4.855150391255861",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m _ut_3 \u001b[38;5;241m=\u001b[39m bim_score(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocs/grauniad_news_001.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniversity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAustralia\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m math\u001b[38;5;241m.\u001b[39misclose(_ut_3, \u001b[38;5;241m4.434597\u001b[39m, rel_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ut_3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Incorrect score: 4.855150391255861"
     ]
    }
   ],
   "source": [
    "_ut_3 = bim_score('docs/grauniad_news_001.txt', ['university', 'Australia'])\n",
    "assert math.isclose(_ut_3, 4.434597, rel_tol=1e-6), f\"Incorrect score: {_ut_3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793c822-af79-4850-b56b-3ad54abb9674",
   "metadata": {},
   "source": [
    "4. Create a function named `bim_rank()` that takes a list of query terms, a count `k` (default 10), and returns a ranked list of the top `k` relevant documents from the `docs` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "357b2f0d-66c7-420b-8890-fa159a9f2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def bim_rank(terms, k = 10):\n",
    "    ranking = []\n",
    "    for doc in Path(f\"docs/\").iterdir():\n",
    "        if doc.is_file():\n",
    "            ranking.append( (bim_score(doc, terms), str(doc)) )\n",
    "\n",
    "    return sorted(ranking, reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82f0e1-209c-480c-99b7-68d417e988c2",
   "metadata": {},
   "source": [
    "5. Call your `bim_rank()` function with the terms \"university\" and \"Australia\". Assign the results to a variable named `bim_test` and state the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1a817f-1a42-40fa-bdbc-60d49a683e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.855150391255861, 'docs\\\\nytimes_sports2_002.txt'),\n",
       " (4.855150391255861, 'docs\\\\grauniad_news_001.txt'),\n",
       " (2.4849066497880004, 'docs\\\\nytimes_sports2_001.txt'),\n",
       " (2.4849066497880004, 'docs\\\\nytimes_news_005.txt'),\n",
       " (2.4849066497880004, 'docs\\\\nytimes_news2_002.txt'),\n",
       " (2.4849066497880004, 'docs\\\\nytimes_columns2_000.txt'),\n",
       " (2.4849066497880004, 'docs\\\\medicine_001.txt'),\n",
       " (2.4849066497880004, 'docs\\\\grauniad_columns_002.txt'),\n",
       " (2.3702437414678603, 'docs\\\\nytimes_sports2_005.txt'),\n",
       " (2.3702437414678603, 'docs\\\\nytimes_columns2_001.txt')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bim_test = bim_rank([\"university\", \"Australia\"])\n",
    "bim_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e455892-1323-42a0-a2eb-a587351316ab",
   "metadata": {},
   "source": [
    "6. Review the top 2 relevant documents. Do you think they are _topically_ relevant and _user_ relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dee3d-3b5a-45ee-b4df-5dcf9a621908",
   "metadata": {},
   "source": [
    "The top two documents are not really topically relevant to the query terms “university” and “Australia.”\n",
    "1. The first document is about America’s Cup sailing and only briefly mentions New York University.\n",
    "2. The second is about Iraq’s political situation with no connection to either term.\n",
    "\n",
    "Because of that, they are also not user-relevant, since they have nothing to do wiht universities or Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9551d-3796-45f4-8c18-5f230996cf44",
   "metadata": {},
   "source": [
    "### Part II: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6500a-ba39-4e66-9098-80844b02071f",
   "metadata": {},
   "source": [
    "The BM25 algorithm modifies the _binary independence model_ by adding what amounts to term frequency information. Again, using the assumption that information about the relvant set is unavailable it reduces to:\n",
    "\n",
    "$$ BM25(Q,d) = \\sum_{i \\in Q} \\log \\frac{1}{(n_i + 0.5)/(N - n_i + 0.5)} \\frac{(k_1 + 1)f_i}{K + f_i} \\frac{ (k_2 + 1) qf_i}{k_2 + qf_i} $$\n",
    "\n",
    "where $f_i$ is the frequency of term $i$ in the document and $qf_i$ is the frequency of term $i$ in the query (usually just 1). $K$, $k_1$, and $k_2$ are parameters set empirically and\n",
    "\n",
    "$$ K = k_1((1 - b) + b \\frac{dl}{avdl}) $$\n",
    "\n",
    "where $b$ is a parameter, $dl$ is the length of the document, and $avdl$ is the average length of a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b78c49-0e43-4db2-b1a9-3c0771c378b7",
   "metadata": {},
   "source": [
    "Since we already have $N$ and $n_i$ from part I, we'll start with $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd0069-8a29-42e3-b1dc-d1186454d4bd",
   "metadata": {},
   "source": [
    "7. Create a variable called `avdl` which is the average document length (words) of the documents in the collection (the `docs` directory). State the value of `avdl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "576b68b4-63e5-40b1-8935-869c52358d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059.4741379310344"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "avd1 = 0\n",
    "count =0\n",
    "for doc in Path(\"docs/\").iterdir():\n",
    "    if doc.is_file():\n",
    "        count += 1 \n",
    "        with open(doc) as file:\n",
    "            avd1 += len (nltk.tokenize.word_tokenize(file. read()))\n",
    "\n",
    "avd1 = avd1 / count\n",
    "avd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7b92b-9b6d-4ad1-a03f-3903ee1b64d7",
   "metadata": {},
   "source": [
    "8. Create a function named `get_K()` which takes a parameter of the document length (in words) and the values for $k1$ and $b$, and returns the value of $K$ in the BM25 equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52a6cd42-ebca-4e50-b779-56404d35efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K(d1, k1, b):\n",
    "    return k1 * ((1 - b) +b * (d1 / avd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcfd1472-7fa0-48f2-927d-08b086087bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ut_8 = get_K(1000, 1.2, 0.75)\n",
    "assert math.isclose(_ut_8, 1.149478, rel_tol=1e-6), f\"Incorrect K: {_ut_8}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e338d-8e8d-4702-8ba3-8feaee0b5f75",
   "metadata": {},
   "source": [
    "9. Create a function named `term_weight()` with a string parameter `term`, an integer parameter `f`, and float parameters `K`, `k1`, and `k2`. The function should calculate the weight for a single term in the BM25 equation. Assume that $qf_i$ = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02c9159c-4473-4d9a-a51b-493a358323a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_weight(term, f, K, k1, k2):\n",
    "    n = term_count(term)\n",
    "    x = (N - n + 0.5) / (n + 0.5)\n",
    "    y = ((k1 + 1) *  f) / (K + f)\n",
    "    z = (k2 + 1) * 1 / (k2 + 1)\n",
    "    return math.log(x) * y * z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bf214-0248-4b03-816f-284ab6bc4b24",
   "metadata": {},
   "source": [
    "10. Create a function named `bm25_score()` which takes as parameters a document filename, a list of query terms, and values for $b$, $k_1$, and $k_2$. The function should return the BM25 score for the document given the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3eb3eace-7acb-41f0-b605-44663bdca647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score(doc, terms, b, k1, k2):\n",
    "    with open(doc) as file:\n",
    "        dt = nltk.Text(nltk.tokenize.word_tokenize(file.read().lower()))\n",
    "    dl = len(dt)\n",
    "    K = get_K(dl, k1, b)\n",
    "\n",
    "    score = 0\n",
    "    for term in terms:\n",
    "        f = dt.count(term.lower())\n",
    "        tw = term_weight(term.lower(), f , K, k1, k2)\n",
    "        score += tw\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc98024-0aed-46ff-be36-66d46705148e",
   "metadata": {},
   "source": [
    "11. Create a function named `bm25_rank()` that takes a list of query terms, values for $b$, $k1$, $k2$, and a count `k` (default 10), and returns a ranked list of the top `k` relevant documents from the `docs` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a34ae75-852c-46ce-8f79-85bd582998ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rank(terms, b, k1, k2, k = 10):\n",
    "    ranking = []\n",
    "    for doc in Path(\"docs/\").iterdir():\n",
    "        if doc.is_file():\n",
    "            ranking.append( ( bm25_score(doc, terms, b, k1, k2), str(doc)) )\n",
    "    return sorted(ranking, reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344cd80-a9c0-4a08-90db-eb6343f805d4",
   "metadata": {},
   "source": [
    "12. Call your `bm25_rank()` function with the terms \"university\" and \"Australia\" using `b=0.75`, `k1=1.2`, and `k2=100`. Assign the result to a variable named `bm25_test` and state the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff41cdf0-ef35-4fd6-b404-077e4af98913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9.23220765060729, 'docs\\\\grauniad_news_001.txt'),\n",
       " (5.120368151909217, 'docs\\\\nytimes_sports2_002.txt'),\n",
       " (4.6054529297340085, 'docs\\\\grauniad_columns_002.txt'),\n",
       " (4.462219742866657, 'docs\\\\guardian_sports_003.txt'),\n",
       " (4.434002614725485, 'docs\\\\grauniad_sports_000.txt'),\n",
       " (4.1909019557522145, 'docs\\\\nytimes_news2_002.txt'),\n",
       " (3.907832104077954, 'docs\\\\guardian_sports_004.txt'),\n",
       " (2.6189750674231216, 'docs\\\\nytimes_sports2_001.txt'),\n",
       " (2.5256406777863285, 'docs\\\\nytimes_news_005.txt'),\n",
       " (2.456513182785306, 'docs\\\\nytimes_columns2_001.txt')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_test = bm25_rank([\"university\", \"Australia\"], 0.75, k1 = 1.2, k2 = 100)\n",
    "bm25_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a7db0-bf0e-4dec-b6ea-db1583d1c4cd",
   "metadata": {},
   "source": [
    "13. Iterate over the length of `bm25_test` and print the tuples from `bim_test` side-by-side on a single line for each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d4fb78e-cb87-40e4-85d6-c7171e103201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.23220765060729, 'docs\\\\grauniad_news_001.txt') (4.855150391255861, 'docs\\\\nytimes_sports2_002.txt')\n",
      "(5.120368151909217, 'docs\\\\nytimes_sports2_002.txt') (4.855150391255861, 'docs\\\\grauniad_news_001.txt')\n",
      "(4.6054529297340085, 'docs\\\\grauniad_columns_002.txt') (2.4849066497880004, 'docs\\\\nytimes_sports2_001.txt')\n",
      "(4.462219742866657, 'docs\\\\guardian_sports_003.txt') (2.4849066497880004, 'docs\\\\nytimes_news_005.txt')\n",
      "(4.434002614725485, 'docs\\\\grauniad_sports_000.txt') (2.4849066497880004, 'docs\\\\nytimes_news2_002.txt')\n",
      "(4.1909019557522145, 'docs\\\\nytimes_news2_002.txt') (2.4849066497880004, 'docs\\\\nytimes_columns2_000.txt')\n",
      "(3.907832104077954, 'docs\\\\guardian_sports_004.txt') (2.4849066497880004, 'docs\\\\medicine_001.txt')\n",
      "(2.6189750674231216, 'docs\\\\nytimes_sports2_001.txt') (2.4849066497880004, 'docs\\\\grauniad_columns_002.txt')\n",
      "(2.5256406777863285, 'docs\\\\nytimes_news_005.txt') (2.3702437414678603, 'docs\\\\nytimes_sports2_005.txt')\n",
      "(2.456513182785306, 'docs\\\\nytimes_columns2_001.txt') (2.3702437414678603, 'docs\\\\nytimes_columns2_001.txt')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bm25_test)):\n",
    "    print(bm25_test[i], bim_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128f047-6423-4420-bfda-51238c1de1f1",
   "metadata": {},
   "source": [
    "14. Compare and analyze the rankings from the two algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b56b7-7f00-49dd-bf94-8f2d45398471",
   "metadata": {},
   "source": [
    "For this query the two models are mostly similar: both BIM and BM25 put grauniad_news_001.txt and nytimes_sports2_002.txt at the very top, and the rest of the list is almost the same set of documents, just in a different order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527477c-8919-4728-8449-61e46fbb5ba0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45994ae463ac93e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136280fa-26d9-4906-b0ae-7d4f884bc73d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bc6d0417687cb84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Be sure to ***SAVE YOUR WORK***!  \n",
    "\n",
    "Next, select Kernel -> Restart Kernel and Run All Cells...\n",
    "\n",
    "Make sure there are no errors.\n",
    "\n",
    "Use _File > Save and Export Notebook As > HTML_ then submit your HTML file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
