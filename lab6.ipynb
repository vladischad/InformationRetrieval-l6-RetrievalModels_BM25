{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60c472b-a00e-49e1-a13b-298d3584fbc3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25c41159fbc3b698",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 6 - Retrieval Models: BM25\n",
    "CS 437  \n",
    "Fall 2025  \n",
    "Dr. Henderson  \n",
    "_v1_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23696b4f-ffc7-47b7-9bca-deec5ad6fe4a",
   "metadata": {},
   "source": [
    "The BM25 ranking model is one of the most effective models assuming a binary relevance. It is based on the _binary independence model_ which makes the Naive Bayes assumption of _term independence_, simplifying the calculations.\n",
    "\n",
    "Although the _binary independence model_ is known to have poor performance, we'll begin by implementing it so we can extend it to BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6534ae0-af67-49db-bf44-b13c1905fd60",
   "metadata": {},
   "source": [
    "The score for a document in the _binary independence model_ is given by\n",
    "$$ bim\\_score(d) = \\sum_{i:d_i=1} \\log \\frac{p_i(1-s_i)}{s_i(1-p_i)} $$\n",
    "\n",
    "Where $d_i=1$ means term $i$ is present in the document, $p_i$ is the probability term $i$ occurs in the relevant set, and $s_i$ is the probability term $i$ does _not_ occur in the relevant set.\n",
    "\n",
    "Since we don't know the relevant set beforehand we cannot calculate $p_i$ and $s_i$ directly so we'll use the estimates as described in the book. We'll choose $p_i = 0.5$ (50/50 chance the term occurs in the relevant set), and $s_i = \\frac{n_i}{N}$ (proportion of documents the term occurs in the whole collection $N$) giving us:\n",
    "$$ bim\\_score(d) = \\sum_{i:d_i=1} \\log \\frac{N - n_i}{n_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788e5c1-ee39-4474-8934-e4e11f8e3cab",
   "metadata": {},
   "source": [
    "_Note: We won't preprocess the query or the corpus for this lab to make it simpler_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826478a-d998-498c-8b6e-d3854e793711",
   "metadata": {},
   "source": [
    "### Part 1: Binary Independece Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d936989-e2f6-4217-9d2e-11b387dfec13",
   "metadata": {},
   "source": [
    "1. Create a function called `term_count()` which takes a string and returns the number of documents it occurs in the collection found in the `docs` subdirectory.  \n",
    "   _Hint: In homework 1 you used a system command to quickly search a set of documents for a term_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5b8561-aaa7-49a5-a255-1cb9794f5d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46196f61-7df4-4d01-9e8c-0393f81bccb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1094c683-b58e-4a71-b898-b66dcc4ceba7",
   "metadata": {},
   "source": [
    "2. Set a variable named `N` to the number of documents in the `docs` subdirectory, and state `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850351d-b8cd-402a-86f0-8a846ec43fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac15b87-c095-4b67-9c8b-b1476f96b5cd",
   "metadata": {},
   "source": [
    "3. Create a function named `bim_score()` which takes a document filename and a list of query terms and returns the BIM score for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d6345-2ce7-4c60-aff9-0d28171b62c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "911687df-1ac4-40a0-8aa3-5a1ed8ef2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ut_3 = bim_score('docs/grauniad_news_001.txt', ['university', 'Australia'])\n",
    "assert math.isclose(_ut_3, 4.434597, rel_tol=1e-6), f\"Incorrect score: {_ut_3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793c822-af79-4850-b56b-3ad54abb9674",
   "metadata": {},
   "source": [
    "4. Create a function named `bim_rank()` that takes a list of query terms, a count `k` (default 10), and returns a ranked list of the top `k` relevant documents from the `docs` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b2f0d-66c7-420b-8890-fa159a9f2759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a82f0e1-209c-480c-99b7-68d417e988c2",
   "metadata": {},
   "source": [
    "5. Call your `bim_rank()` function with the terms \"university\" and \"Australia\". Assign the results to a variable named `bim_test` and state the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a817f-1a42-40fa-bdbc-60d49a683e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e455892-1323-42a0-a2eb-a587351316ab",
   "metadata": {},
   "source": [
    "6. Review the top 2 relevant documents. Do you think they are _topically_ relevant and _user_ relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dee3d-3b5a-45ee-b4df-5dcf9a621908",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c9551d-3796-45f4-8c18-5f230996cf44",
   "metadata": {},
   "source": [
    "### Part II: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6500a-ba39-4e66-9098-80844b02071f",
   "metadata": {},
   "source": [
    "The BM25 algorithm modifies the _binary independence model_ by adding what amounts to term frequency information. Again, using the assumption that information about the relvant set is unavailable it reduces to:\n",
    "\n",
    "$$ BM25(Q,d) = \\sum_{i \\in Q} \\log \\frac{1}{(n_i + 0.5)/(N - n_i + 0.5)} \\frac{(k_1 + 1)f_i}{K + f_i} \\frac{ (k_2 + 1) qf_i}{k_2 + qf_i} $$\n",
    "\n",
    "where $f_i$ is the frequency of term $i$ in the document and $qf_i$ is the frequency of term $i$ in the query (usually just 1). $K$, $k_1$, and $k_2$ are parameters set empirically and\n",
    "\n",
    "$$ K = k_1((1 - b) + b \\frac{dl}{avdl}) $$\n",
    "\n",
    "where $b$ is a parameter, $dl$ is the length of the document, and $avdl$ is the average length of a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b78c49-0e43-4db2-b1a9-3c0771c378b7",
   "metadata": {},
   "source": [
    "Since we already have $N$ and $n_i$ from part I, we'll start with $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd0069-8a29-42e3-b1dc-d1186454d4bd",
   "metadata": {},
   "source": [
    "7. Create a variable called `avdl` which is the average document length (words) of the documents in the collection (the `docs` directory). State the value of `avdl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b68b4-63e5-40b1-8935-869c52358d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11b7b92b-9b6d-4ad1-a03f-3903ee1b64d7",
   "metadata": {},
   "source": [
    "8. Create a function named `get_K()` which takes a parameter of the document length (in words) and the values for $k1$ and $b$, and returns the value of $K$ in the BM25 equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6cd42-ebca-4e50-b779-56404d35efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfd1472-7fa0-48f2-927d-08b086087bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ut_8 = get_K(1000, 1.2, 0.75)\n",
    "assert math.isclose(_ut_8, 1.149478, rel_tol=1e-6), f\"Incorrect K: {_ut_8}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e338d-8e8d-4702-8ba3-8feaee0b5f75",
   "metadata": {},
   "source": [
    "9. Create a function named `term_weight()` with a string parameter `term`, an integer parameter `f`, and float parameters `K`, `k1`, and `k2`. The function should calculate the weight for a single term in the BM25 equation. Assume that $qf_i$ = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9159c-4473-4d9a-a51b-493a358323a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8bf214-0248-4b03-816f-284ab6bc4b24",
   "metadata": {},
   "source": [
    "10. Create a function named `bm25_score()` which takes as parameters a document filename, a list of query terms, and values for $b$, $k_1$, and $k_2$. The function should return the BM25 score for the document given the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3eace-7acb-41f0-b605-44663bdca647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc98024-0aed-46ff-be36-66d46705148e",
   "metadata": {},
   "source": [
    "11. Create a function named `bm25_rank()` that takes a list of query terms, values for $b$, $k1$, $k2$, and a count `k` (default 10), and returns a ranked list of the top `k` relevant documents from the `docs` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34ae75-852c-46ce-8f79-85bd582998ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6344cd80-a9c0-4a08-90db-eb6343f805d4",
   "metadata": {},
   "source": [
    "12. Call your `bm25_rank()` function with the terms \"university\" and \"Australia\" using `b=0.75`, `k1=1.2`, and `k2=100`. Assign the result to a variable named `bm25_test` and state the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41cdf0-ef35-4fd6-b404-077e4af98913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5a7db0-bf0e-4dec-b6ea-db1583d1c4cd",
   "metadata": {},
   "source": [
    "13. Iterate over the length of `bm25_test` and print the tuples from `bim_test` side-by-side on a single line for each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fb78e-cb87-40e4-85d6-c7171e103201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c128f047-6423-4420-bfda-51238c1de1f1",
   "metadata": {},
   "source": [
    "14. Compare and analyze the rankings from the two algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b56b7-7f00-49dd-bf94-8f2d45398471",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7527477c-8919-4728-8449-61e46fbb5ba0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45994ae463ac93e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136280fa-26d9-4906-b0ae-7d4f884bc73d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bc6d0417687cb84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Be sure to ***SAVE YOUR WORK***!  \n",
    "\n",
    "Next, select Kernel -> Restart Kernel and Run All Cells...\n",
    "\n",
    "Make sure there are no errors.\n",
    "\n",
    "Use _File > Save and Export Notebook As > HTML_ then submit your HTML file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
